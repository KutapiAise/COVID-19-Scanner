{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tnsorflow 2.0: Crash course.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+0X44OUcLVM9YN9Z2UF1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KutapiAise/COVID-19-Scanner/blob/master/Tensorflow2.0/Tensorflow_2_0_Crash_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAXmUTUTNFdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCNpd9-0NT-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data =keras.datasets.imdb\n",
        "\n",
        "# here the  num_words=10000 picks up the 10000 most frequent words from the imdb dataset, hence shinks thedata\n",
        "(train_data,train_labels),(test_data,test_labels) = data.load_data(num_words=88000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlZsjZv-OC3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5d2ba6c3-cdfe-43e3-da3d-d70fc3f7caac"
      },
      "source": [
        "print(train_data[7])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572, 24785, 43394, 4, 10993, 628, 7685, 37, 9, 150, 4, 9820, 4069, 11, 2909, 4, 16287, 847, 313, 6, 176, 63860, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 39455, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 30497, 6293, 345, 4804, 37377, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 68395, 5262, 63370, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 20636, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 46035, 8, 783, 48545, 33, 4, 2945, 103, 465, 16454, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 14906, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 8361, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 11100, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 11871, 1703, 56, 8, 803, 1004, 6, 18763, 155, 11, 4, 14906, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 8941, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 21486, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 10399, 367, 45, 115, 93, 788, 121, 4, 14906, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 12370, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 8651, 189, 12, 43, 127, 6, 394, 292, 7, 8253, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 11890, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 10373, 12958, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 14906, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 53728, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 14906, 6847, 4, 14065, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 15222, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 8848, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_2nbIuxRvHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index=data.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbOkZx-zR4Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index= {k:(v+3) for k,v in word_index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axCtCVSiUK6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index[\"<PAD>\"] =0\n",
        "word_index[\"<START>\"] =1\n",
        "word_index[\"<UNK>\"] =2\n",
        "word_index[\"<UNUSED>\"] =3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js6CdVsXUsjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_idex=dict([(v,k) for (k,v) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5qjT3rnXc1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "  return \" \".join([reverse_word_idex.get(i,\"?\")  for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfVL4kGNYCZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fefb1506-12fe-474b-e758-06e1360580d0"
      },
      "source": [
        "print(decode_review(test_data[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdO06_1bYIJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "72cc8606-3dc8-4356-ac47-322a704a0546"
      },
      "source": [
        "# to chek the length of each text data\n",
        "print(len(train_data[0]), len(train_data[1])) # we see that the input sizes are not definite in all the data texts"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "218 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QKWR_akn45R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data =keras.preprocessing.sequence.pad_sequences(train_data,value=word_index[\"<PAD>\"], padding=\"post\",maxlen=250)\n",
        "test_data =keras.preprocessing.sequence.pad_sequences(test_data,value=word_index[\"<PAD>\"], padding=\"post\",maxlen=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X16cvA3mnWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0da3a697-9b07-4b28-b9ff-efbfbf6b343d"
      },
      "source": [
        "print(len(train_data[0]), len(train_data[1]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250 250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4KTwu7sAqG",
        "colab_type": "text"
      },
      "source": [
        "What is an embedding layer?\n",
        "\n",
        "conside 2 similar sentences in a context:\n",
        "\n",
        "0   1   2    3\n",
        "\n",
        "HAVE A GREAT DAY  \n",
        "\n",
        "0    1   4   3\n",
        "\n",
        "HAVE A GOOD DAY\n",
        "\n",
        "Here \"GREAT\" and \"GOOD\" mean the same in a context. However, when these are encoded by th computer, it doesn't know that they mean the same.\n",
        "\n",
        "what embeddin layer does?\n",
        "- converts the inputs (10,000  in no) to vectorized form, then group the words that are similar (ie, similar words of the context have small angle b/w them) and ungroup the words that have diferent meaning in the same context.\n",
        "-the vector created by words have certain dimension (ex 16). So the vectorized  form of the words will have the form of an array with 16 elements where each element denoting magnitude in that particuar unit vector drection.\n",
        "\n",
        "- vector rep of a word  [0.2 ,0.75,.......,0.8] (16 elements)\n",
        "\n",
        "- The output of the embedding layer will therefore contain the same number  of values as the input layer with them encoded in the vector format.\n",
        "\n",
        "What does globalaveragepooling1d do?\n",
        "\n",
        "- it scals down the input vector in number. for ex. if the number of input is equal to 10,000 then the output of globalavragepooling layer will be lower than that.\n",
        "\n",
        "What does dense layer do?\n",
        "\n",
        "it looks for patterens in the word vecors and classify them to pos or neg review\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLGYW85sp0Rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b213f076-912f-460d-ba49-b4a159714db8"
      },
      "source": [
        "# model down here\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(88000,16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16,activation='relu'))\n",
        "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          1408000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,408,289\n",
            "Trainable params: 1,408,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leVm-DQ4Ywyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28243716-1cd7-42b6-8bc0-ad04104349d7"
      },
      "source": [
        "model.compile(optimizer=\"adam\"  ,loss='binary_crossentropy' , metrics=[\"accuracy\"],)\n",
        "\n",
        "\n",
        "#validation data\n",
        "\n",
        "x_val =train_data[:10000]\n",
        "x_train =train_data[10000:]\n",
        "\n",
        "y_val =train_labels[:10000]\n",
        "y_train= train_labels[10000:]\n",
        "\n",
        "fitModel = model.fit(x_train,y_train, epochs=40, batch_size =512, validation_dat=(x_val,y_val), verbose =1)\n",
        "\n",
        "results =  model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.6918 - accuracy: 0.5097\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 1s 27ms/step - loss: 0.6865 - accuracy: 0.6253\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.6738 - accuracy: 0.7665\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.6515 - accuracy: 0.7819\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.6167 - accuracy: 0.8035\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.5711 - accuracy: 0.8291\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.5177 - accuracy: 0.8454\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.4647 - accuracy: 0.8615\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.4162 - accuracy: 0.8784\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.3706 - accuracy: 0.8928\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.3330 - accuracy: 0.9025\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.3004 - accuracy: 0.9100\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.2737 - accuracy: 0.9191\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.2495 - accuracy: 0.9254\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.2275 - accuracy: 0.9331\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.2101 - accuracy: 0.9385\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1922 - accuracy: 0.9442\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.1776 - accuracy: 0.9495\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.1660 - accuracy: 0.9539\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1517 - accuracy: 0.9593\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1409 - accuracy: 0.9647\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1307 - accuracy: 0.9677\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.1206 - accuracy: 0.9706\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.1123 - accuracy: 0.9738\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.1043 - accuracy: 0.9767\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0967 - accuracy: 0.9791\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 1s 30ms/step - loss: 0.0903 - accuracy: 0.9811\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0850 - accuracy: 0.9825\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0792 - accuracy: 0.9849\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0732 - accuracy: 0.9865\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0682 - accuracy: 0.9875\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0639 - accuracy: 0.9886\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0600 - accuracy: 0.9892\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0570 - accuracy: 0.9901\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0529 - accuracy: 0.9913\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 1s 28ms/step - loss: 0.0494 - accuracy: 0.9919\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0465 - accuracy: 0.9929\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0437 - accuracy: 0.9937\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0409 - accuracy: 0.9944\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.0385 - accuracy: 0.9947\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3328 - accuracy: 0.8728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo3Ku4gkJi-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f33529b5-c7e0-4778-ee69-a1431b79e2ab"
      },
      "source": [
        "print(results) # we get a list of loss, accuracy"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.3327729105949402, 0.8727999925613403]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBTzQuQfNtnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4kX_TInUfmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def review_encode(s):\n",
        "  encoded= [1] # adiing a start tag <START> to be consistant with  the training data\n",
        "\n",
        "  for word in s:\n",
        "    if word.lower() in word_index:\n",
        "      encoded.append(word_index[word.lower()])\n",
        "    else:\n",
        "      encoded.append(2)\n",
        "\n",
        "    return encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IIdU4UbPH6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ec2461a6-7ade-41c6-82c1-b06e4ad09e58"
      },
      "source": [
        "with open(\"test.txt\", encoding=\"utf-8\") as f:\n",
        "  for line in f.readlines():\n",
        "    nline=line.replace(\",\",\"\").replace(\".\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\":\",\"\").replace(\"\\\"\",\"\").strip().split()\n",
        "    encode1 = review_encode(nline)\n",
        "    encode = keras.preprocessing.sequence.pad_sequences([encode1],value=word_index[\"<PAD>\"], padding=\"post\",maxlen=250)\n",
        "    predict = model.predict(encode)\n",
        "    print(nline)\n",
        "    print(encode1)\n",
        "    print(encode)\n",
        "    print(predict[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'liked', 'the', 'first', 'two', 'movies', 'both', 'had', 'a', 'first', 'behind', 'all', 'the', 'action', 'that', 'takes', 'place', 'this', 'movie', 'the', 'plot', 'is', 'weak', 'and', 'the', 'fight', 'scenes', 'way', 'too', 'long', 'for', 'the', 'weak', 'plot', 'Keanu', 'still', 'portrays', 'his', 'character', 'as', 'the', 'Boogeyman', 'really', 'well', 'I', 'enjoyed', 'it', 'but', 'just', 'felt', 'it', 'dragged', 'on', 'a', 'bit', 'much', 'and', 'the', 'fight', 'scene', 'especially', 'with', 'Halle', 'could', 'have', 'been', 'cut', 'down']\n",
            "[1, 13]\n",
            "[[ 1 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0]]\n",
            "[0.61313635]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q6W1xf6KGBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "480afd9b-cc6a-46c6-8761-8918c3ecc898"
      },
      "source": [
        "#let us  actualy test some of the predictions made mby the model\n",
        "\n",
        "test_review = test_data[0]\n",
        "predict= model.predict([test_review])\n",
        "print(\"Review: \")\n",
        "print(decode_review(test_review))\n",
        "print(\"prediction: \"+ str(predict[0]))\n",
        "print(\"Actual: \"+ str(test_labels[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: \n",
            "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "prediction: [3.0981528e-15]\n",
            "Actual: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5oBXQ_mMbkL",
        "colab_type": "text"
      },
      "source": [
        "SAVING AND LOADING MODELS\n",
        "\n",
        "\n",
        "- save the model == done after complete training the model\n",
        "\n",
        "- checkpoint the mosel == \n",
        "\n",
        " We can also checkpoint the model to save the mdel during its training. This helps the models to resume training at later point of time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DpswgmOL53p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}